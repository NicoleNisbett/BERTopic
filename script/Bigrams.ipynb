{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import collections\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadPath = \"/Users/ipinni/Library/CloudStorage/OneDrive-UniversityofLeeds/UKRI_Tweet_Data/completed/\"\n",
    "savePath = \"/Users/ipinni/Library/CloudStorage/OneDrive-UniversityofLeeds/UKRI_Tweet_Data/BigramAnalysis/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May need to edit the stopwords and run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stopwords_en = set(nltk.corpus.stopwords.words('english'))\n",
    "my_stopwords = set([\"rt\",\"RT\", \"&\", \"amp\", \"&amp\", \"http\",\"https\", \"http://\", \"https://\", \"fav\", \"FAV\", \"cop\",\"@cop20\", \"@cop21\", \"@cop22\", \"@cop23\", \"@cop24\", \"@cop25\",\"@cop26\", \"#cop20\", \"#cop21\", \"#cop22\", \"#cop23\", \"#cop24\", \"#cop25\",\"#cop26\", \"20\", \"21\",\"22\",\"23\",\"24\",\"25\",\"26\", \"u\", \"...\", \"@\"])\n",
    "new_stops = default_stopwords_en | my_stopwords\n",
    "filter_stops = lambda w: len(w)< 2 or w in new_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()     \n",
    "\n",
    "def get_words(tweets):\n",
    "    tweets_text = tweets['text'].apply(tt.tokenize)     \n",
    "    flat_list = [item for sublist in tweets_text for item in sublist] \n",
    "    words = [word for word in flat_list if len(word) > 1]     \n",
    "    words = [word.lower() for word in words]     \n",
    "    words = [word for word in words if word not in new_stops]\n",
    "    words = [word for word in words if '@' not in word]\n",
    "    #lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(topics, docs, choices):\n",
    "    topic_docs = {topic: [] for topic in set(topics)}\n",
    "    for topic, doc in zip(topics, docs):\n",
    "        topic_docs[topic].append(doc)\n",
    "    \n",
    "    chosen_topics = []\n",
    "    for i in range(len(choices)):\n",
    "        chosen_topics.extend(topic_docs[choices[i]])\n",
    "    \n",
    "    tweets = pd.DataFrame(chosen_topics, columns=['text'])\n",
    "\n",
    "    return(tweets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frames = red, Delay = blue, fringe = green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP20_frames = [3,4,5,7,14,22,26,40,41,42,44]\n",
    "#COP20_delay = [6,15,17,31,45]\n",
    "#COP20_fringe = [2,26,41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP21_frames = [3,4,9,14,15,19,29]\n",
    "#COP21_delay = [35,36]\n",
    "#COP21_fringe = [4,12,17,22,23,26,28,34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP22_frames = [13,15,16,17,18,28]\n",
    "#COP22_delay = [20,43,48,52,55]\n",
    "#COP22_fringe = [1,7,8,14,15,33,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP23_frames = [4,13,17,18,31]\n",
    "#COP23_delay = []\n",
    "#COP23_fringe = [1,4,5,14,26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP24_frames = [2,8,17,20,21,26,35,43,44,46,48,51,55,56]\n",
    "#COP24_delay = [22]\n",
    "#COP24_fringe = [13,21,23,25,28,33,34,36,38,47,48,49,54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP25_frames = [3,4,7,8,16,18,26,32,35,43,65]\n",
    "#COP25_delay = [21,36,55]\n",
    "#COP25_fringe = [9,12,13,15,16,19,30,32,33,34,44,48,49,62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "COP26_frames = [3,5,10,23,27,36,37]\n",
    "#COP26_delay = [31]\n",
    "#COP26_fringe = [1,3,5,6,8,15,16,18,21,23,30,34,39,40,41,42,44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 10, 20, 23, 27, 36, 37]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COP26_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF2018_frames = [i for i in range(43)]\n",
    "FFF2019_frames = [i for i in range(55)]\n",
    "FFF2020_frames = [i for i in range(51)]\n",
    "FFF2021_frames = [i for i in range(44)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[2,6,16,23,24,29,32]\n",
    "FFF2018_delay = [11,17,36,41]\n",
    "FFF2018_fringe = [1,4,5,7,10,15,20,22,26,27,28,31,33,37,38,39,42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF2019_frames = [9,12,20,38,40,47]\n",
    "FFF2019_delay = [6,16,31,37,46]\n",
    "FFF2019_fringe = [1,2,3,8,10,18,24,28,29,30,32,34,49,54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF2020_frames = [10,18,19,28,33,38]\n",
    "FFF2020_delay = [45]\n",
    "FFF2020_fringe = [2,4,6,7,8,12,14,17,20,22,23,29,30,37,46,49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFF2021_frames = [6,14,19,21,26,27,32]\n",
    "FFF2021_delay = [22,23,29,30,33,40]\n",
    "FFF2021_fringe = [1,2,3,4,5,7,8,10,17,25,28,41,42,43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value of COPversion and runn all cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPversion = \"COP26\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    with open(loadPath + COPversion + \"/\" + COPversion + \"topics.list\" ,'rb') as config_list_file:   \n",
    "        topics = pickle.load(config_list_file)\n",
    "\n",
    "    with open(loadPath + COPversion + \"/\" + COPversion + \"docs.list\", 'rb') as docs_list_file:   \n",
    "        docs = pickle.load(docs_list_file)\n",
    "\n",
    "    return(topics, docs)\n",
    "\n",
    "topics, docs = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = get_topics(topics, docs, eval((COPversion+\"_frames\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biFrames(text):\n",
    "    bgm = nltk.collocations.BigramAssocMeasures()\n",
    "    bcf = nltk.collocations.BigramCollocationFinder.from_words(text)\n",
    "    bcf.apply_word_filter(filter_stops)\n",
    "    scored = bcf.score_ngrams(bgm.student_t)[:750]\n",
    "    temp_dict = dict(scored)\n",
    "    csvData = [\"Source, Target, Weight\"]\n",
    "    for (col1, col2), col3 in iter(temp_dict.items()):\n",
    "        csvData.append(\"%s, %s, %s\" % (col1, col2, col3))\n",
    "    f = open(savePath + \"Frames/\" + COPversion + 'BigramFrames' + '.csv', 'w')\n",
    "    f.write(\"\\n\".join(csvData))\n",
    "    f.close()\n",
    "biFrames(get_words(frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = get_topics(topics, docs, eval((COPversion+\"_delay\")))\n",
    "fringe = get_topics(topics, docs, eval((COPversion+\"_fringe\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biDelay(text):\n",
    "    bgm = nltk.collocations.BigramAssocMeasures()\n",
    "    bcf = nltk.collocations.BigramCollocationFinder.from_words(text)\n",
    "    bcf.apply_word_filter(filter_stops)\n",
    "    scored = bcf.score_ngrams(bgm.student_t)[:750]\n",
    "    temp_dict = dict(scored)\n",
    "    csvData = [\"Source, Target, Weight\"]\n",
    "    for (col1, col2), col3 in iter(temp_dict.items()):\n",
    "        csvData.append(\"%s, %s, %s\" % (col1, col2, col3))\n",
    "    f = open(savePath + \"Delay/\" + COPversion + 'BigramDelay' +'.csv', 'w')\n",
    "    f.write(\"\\n\".join(csvData))\n",
    "    f.close()\n",
    "biDelay(get_words(delay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biFringe(text):\n",
    "    bgm = nltk.collocations.BigramAssocMeasures()\n",
    "    bcf = nltk.collocations.BigramCollocationFinder.from_words(text)\n",
    "    bcf.apply_word_filter(filter_stops)\n",
    "    scored = bcf.score_ngrams(bgm.student_t)[:750]\n",
    "    temp_dict = dict(scored)\n",
    "    csvData = [\"Source, Target, Weight\"]\n",
    "    for (col1, col2), col3 in iter(temp_dict.items()):\n",
    "        csvData.append(\"%s, %s, %s\" % (col1, col2, col3))\n",
    "    f = open(savePath + \"Fringe/\" + COPversion + 'BigramFringe' + '.csv', 'w')\n",
    "    f.write(\"\\n\".join(csvData))\n",
    "    f.close()\n",
    "biFringe(get_words(fringe))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe4d8685886068fc374377cb0ce03965843ae3df30816ebe900dd55a8a561255"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('bertopic_env2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
